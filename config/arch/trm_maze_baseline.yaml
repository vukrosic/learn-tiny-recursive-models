# TRM baseline for Maze
# NOTE: Code uses HRM variable names but this is TRM with a SINGLE network
# In TRM paper: z_H=y (current solution), z_L=z (latent reasoning)
# H_cycles = T (deep recursion cycles), L_cycles = n (latent updates per cycle)

name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
loss:
  name: losses@ACTLossHead
  loss_type: stablemax_cross_entropy

halt_exploration_prob: 0.1
halt_max_steps: 16

# TRM uses SINGLE network (L_level), NOT two separate networks like HRM
H_cycles: 3  # T=3: deep recursion cycles (improve answer y this many times)
L_cycles: 6  # n=6: latent recursion steps (update z this many times before updating y)

H_layers: 0  # Not used in TRM (HRM used separate H network, TRM uses single network)
L_layers: 2  # Single 2-layer network used for BOTH z and y updates

hidden_size: 512
num_heads: 8
expansion: 4

puzzle_emb_ndim: ${.hidden_size}

pos_encodings: rope
forward_dtype: bfloat16

mlp_t: False  # use self-attention (not MLP)
puzzle_emb_len: 16
no_ACT_continue: True  # Only use halt signal, no continue loss

