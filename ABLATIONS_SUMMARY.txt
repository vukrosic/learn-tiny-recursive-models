================================================================================
TRM ABLATION STUDY SETUP - MAZE DATASET
================================================================================

IMPORTANT: TRM vs HRM
  - TRM (Tiny Recursive Model) uses a SINGLE network
  - HRM (Hierarchical Reasoning Model) uses TWO networks (fL and fH)
  - Code still uses HRM variable names but implements TRM single-network approach
  - H_cycles = T (deep recursion), L_cycles = n (latent updates)
  - H_layers=0 (no separate network), L_layers=2 (single 2-layer network)

================================================================================

CREATED FILES:
--------------------------------------------------------------------------------

Configuration Files (9 files):
  config/cfg_pretrain_maze.yaml                 - Baseline training config
  config/cfg_pretrain_maze_no_ema.yaml          - No EMA ablation config
  config/cfg_pretrain_maze_with_act.yaml        - With ACT continue config
  config/cfg_pretrain_maze_T2_n2.yaml           - T=2, n=2 ablation config
  config/cfg_pretrain_maze_4layers.yaml         - 4-layers ablation config
  
  config/arch/trm_maze_baseline.yaml            - Baseline architecture
  config/arch/trm_maze_T2_n2.yaml               - T=2, n=2 architecture
  config/arch/trm_maze_4layers_n3.yaml          - 4-layers architecture
  config/arch/trm_maze_with_act_continue.yaml   - With ACT architecture

Scripts (2 files):
  run_maze_ablations.sh                         - Run all experiments
  analyze_maze_ablations.py                     - Analyze results

Documentation (3 files):
  ABLATIONS_QUICKSTART.md                       - Quick start guide
  MAZE_ABLATIONS.md                             - Detailed documentation
  ABLATIONS_SUMMARY.txt                         - This file

================================================================================
EXPERIMENTS OVERVIEW
================================================================================

5 Ablation Experiments:

1. BASELINE TRM
   - Config: cfg_pretrain_maze
   - Architecture: T=3, n=6, 2-layers
   - EMA: Yes
   - ACT Continue: No
   - Effective Depth: 42 layers
   - Expected: Best performance (reference)

2. NO EMA
   - Config: cfg_pretrain_maze_no_ema
   - Architecture: T=3, n=6, 2-layers
   - EMA: No (ABLATION)
   - ACT Continue: No
   - Effective Depth: 42 layers
   - Expected: -7.5% from baseline (training instability)

3. WITH ACT CONTINUE
   - Config: cfg_pretrain_maze_with_act
   - Architecture: T=3, n=6, 2-layers
   - EMA: Yes
   - ACT Continue: Yes (ABLATION)
   - Effective Depth: 42 layers
   - Expected: -1.3% from baseline (2x slower training)

4. T=2, n=2 (LESS RECURSION)
   - Config: cfg_pretrain_maze_T2_n2
   - Architecture: T=2, n=2, 2-layers (ABLATION)
   - EMA: Yes
   - ACT Continue: No
   - Effective Depth: 12 layers
   - Expected: -13.7% from baseline (insufficient recursion)

5. 4-LAYERS, n=3
   - Config: cfg_pretrain_maze_4layers
   - Architecture: T=3, n=3, 4-layers (ABLATION)
   - EMA: Yes
   - ACT Continue: No
   - Effective Depth: 48 layers
   - Expected: -7.9% from baseline (overfitting)

================================================================================
HOW TO RUN
================================================================================

Step 1: Prepare Dataset
  python dataset/build_maze_dataset.py --aug

Step 2: Run Experiments
  
  Option A - Run all experiments:
    bash run_maze_ablations.sh
  
  Option B - Run individual experiment:
    python pretrain.py --config-name=cfg_pretrain_maze
    python pretrain.py --config-name=cfg_pretrain_maze_no_ema
    python pretrain.py --config-name=cfg_pretrain_maze_with_act
    python pretrain.py --config-name=cfg_pretrain_maze_T2_n2
    python pretrain.py --config-name=cfg_pretrain_maze_4layers
  
  Option C - Multi-GPU training:
    torchrun --nproc_per_node=4 pretrain.py --config-name=cfg_pretrain_maze

Step 3: Analyze Results
  python analyze_maze_ablations.py

================================================================================
TRAINING DETAILS
================================================================================

Common Settings:
  - Dataset: Maze-30x30-Hard (1K train, 1K test)
  - Epochs: 60,000
  - Batch Size: 768
  - Learning Rate: 1e-4
  - Weight Decay: 1.0
  - Warmup Steps: 2,000
  - Eval Interval: Every 10,000 epochs
  - Min Eval: Start at 50,000 epochs

Time Estimates (on 4 GPUs):
  - Per experiment: ~24 hours
  - All 5 experiments: ~5 days

================================================================================
EXPECTED RESULTS (based on Sudoku Table 1)
================================================================================

Experiment          | Depth | Params | Expected Δ | Key Finding
--------------------|-------|--------|------------|---------------------------
Baseline TRM        |   42  |  ~7M   |    0.0%    | Reference
No EMA              |   42  |  ~7M   |   -7.5%    | EMA critical for stability
With ACT Continue   |   42  |  ~7M   |   -1.3%    | Simplified ACT works well
T=2, n=2            |   12  |  ~7M   |  -13.7%    | Deep recursion essential
4-layers, n=3       |   48  | ~14M   |   -7.9%    | Less is more (overfitting)

================================================================================
KEY FINDINGS TO VALIDATE
================================================================================

1. EMA is crucial for training stability on small datasets
2. Simplified halting (no continue loss) works nearly as well as full ACT
3. Deep recursion (T=3, n=6) is more important than model capacity
4. Smaller networks (2 layers) + more recursion > larger networks (4 layers)
5. Effective depth matters more than parameter count

================================================================================
FILES REFERENCE
================================================================================

Quick Start:
  ABLATIONS_QUICKSTART.md    - Start here!

Detailed Info:
  MAZE_ABLATIONS.md          - Full experiment documentation
  paper.md                   - Original TRM paper
  README.md                  - Project overview

Run Experiments:
  run_maze_ablations.sh      - Automated runner
  pretrain.py                - Training script

Analyze:
  analyze_maze_ablations.py  - Results comparison

Configs:
  config/cfg_pretrain_maze*.yaml      - Training configs
  config/arch/trm_maze*.yaml          - Architecture configs

================================================================================
MONITORING
================================================================================

Weights & Biases Dashboard:
  - Train/test loss curves
  - Test accuracy over time
  - Average halting steps
  - Learning rate schedule

Key Metrics:
  - test/accuracy       : Final test accuracy (main metric)
  - train/loss          : Training loss
  - train/avg_steps     : Average halting steps (should be < 16)
  - train/lr            : Current learning rate

================================================================================
TROUBLESHOOTING
================================================================================

Problem: Out of memory
  Solution: Reduce global_batch_size (768 → 384)

Problem: Dataset not found
  Solution: python dataset/build_maze_dataset.py --aug

Problem: Slow training
  Solution: Use torchrun with multiple GPUs

Problem: No results in analysis
  Solution: Wait for experiments to complete, check W&B

================================================================================
NEXT STEPS AFTER RESULTS
================================================================================

1. Compare test accuracy across ablations
2. Analyze training curves in W&B
3. Validate findings match paper expectations
4. Consider additional ablations:
   - Different learning rates
   - Different T/n combinations
   - MLP vs self-attention (mlp_t: True/False)
   - Different effective depths

================================================================================
PAPER REFERENCE
================================================================================

Title: Less is More: Recursive Reasoning with Tiny Networks
Author: Alexia Jolicoeur-Martineau
arXiv: 2510.04871
Table: Table 1 (Ablation of TRM on Sudoku-Extreme)

Key Sections:
  - Section 4: Tiny Recursion Models
  - Section 4.1: No fixed-point theorem required
  - Section 4.4: Less is more
  - Section 4.6: No additional forward pass needed with ACT
  - Section 4.7: Exponential Moving Average (EMA)

================================================================================

